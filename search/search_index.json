{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Migration Tools","text":"<p><code>migration_tools</code> is a command line interface to help the ThoughtSpot Professional Services team accelerate migration workflows. Some functionalities require also the installation of cs_tools.</p>"},{"location":"#installing-getting-started","title":"Installing &amp; Getting Started","text":"<p>The <code>migration_tools</code> CLI requires at least <code>python</code> version 3.9 to run.</p> <pre><code>$ python -m venv .ts-mig\n$ source .ts-mig/bin/activate  # or windows:    .ts-mig/Scripts/activate \n(.ts-mig) $ pip install https://github.com/thoughtspot/ps_migration_tools\n</code></pre>"},{"location":"migration-tools/overview/","title":"Migration Tools","text":"<p><code>migration_tools</code> is a command line interface to help the ThoughtSpot Professional Services team accelerate migration workflows. Some functionalities require also the installation of cs_tools.</p>"},{"location":"migration-tools/overview/#installing-getting-started","title":"Installing &amp; Getting Started","text":"<p>The <code>migration_tools</code> CLI requires at least <code>python</code> version 3.9 to run.</p> <pre><code>$ python -m venv .ts-mig\n$ source .ts-mig/bin/activate  # or windows:    .ts-mig/Scripts/activate \n(.ts-mig) $ pip install https://github.com/thoughtspot/ps_migration_tools/ps_migration_tools.zip\n</code></pre>"},{"location":"migration-tools/overview/#tools","title":"Tools","text":""},{"location":"migration-tools/answers/readme/","title":"Migrate Answers","text":"<p>This function migrates all the created/modified answers that have been gathered during the <code>deltas</code> step.</p> <pre><code>usage: migrate_answers [-h] [--cfg-name] [--migration-mode MODE]\n                     [--validation-mode TRUE/FALSE] ...\n\narguments:\n  -h, --help                   show this help message and exit\n  --cfg-name   CONFIGNAME      Name of config.toml file.\n  --migration-mode  TYPE       Specify if you want to migrate created or modified answers. valid values:[created, modified]\n  --validation-mode BOOL       Run in validation mode only or apply actual changes (default: True). Set to False to migrate all objects.\n</code></pre>"},{"location":"migration-tools/answers/readme/#cli-preview","title":"CLI preview","text":"migrate_answers --help"},{"location":"migration-tools/convert-ddls/readme/","title":"Convert DDLs","text":"<p>Use this command to convert sql DDL scripts placed in the input/ddl/ folder to DBSchema model files (.DBS), which can be processed by the validate_models and migrate_yaml commands. The script will place the generated *.DBS files in the input/model/ folder.   <p>You can only use DDL scripts as import if you have DBSchema installed.</p> <p>To use DDL scripts as input you would need to have license to DBSchema and have installed as this will be used to convert the DDLs into a DBSchema model file, which is then processed by validate models command. If you don't have DBSchema, you can use CSV files as input and place them in the input/model/ folder."},{"location":"migration-tools/convert-ddls/readme/#cli-preview","title":"CLI preview","text":"convert_ddls --help"},{"location":"migration-tools/create-config/readme/","title":"Create Config","text":"<p>Sets up the required folder structure for migration tools and creates a template configuration file in the config folder with the name of the project. This .toml file will need to be edited before the first run.</p>"},{"location":"migration-tools/create-config/readme/#folder-structure","title":"Folder structure","text":"<p>The following folder structure will be created under the project folder </p> <pre><code>\u251c\u2500\u2500 &lt;projects&gt;\n\u2502   \u251c\u2500\u2500 &lt;project_name&gt;\n\u2502   \u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 &lt;project_name&gt;.toml\n\u2502   \u2502   \u251c\u2500\u2500 input\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 business_model\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 &lt;business_model&gt;.xls\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 connections_yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 &lt;original_remapping_file&gt;.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ddl\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 falcon\n\u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 **/*.sql\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 target\n\u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 **/*.sql\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 models\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 falcon\n\u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 **/*.dbs\n\u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 **/*.csv\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 target\n\u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 **/*.dbs\n\u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 **/*.csv\n\u2502   \u2502   \u251c\u2500\u2500 log\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 &lt;yyyymmdd_HHMMSS&gt;_migration.log\n\u2502   \u2502   \u251c\u2500\u2500 output\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 delta_migrations\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 cs_tools_cloud\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 cs_tools_falcon\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 modified\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 new\n</code></pre>"},{"location":"migration-tools/create-config/readme/#configuration-file","title":"Configuration file","text":"<p>The process will also create a template configuration file with the same name as the project in the config folder of the created directory structure. Please adjust these parameters according to your needs, althought typically you would only need to change the MIGRATION parameters (source and target platform).</p> <pre><code>[MIGRATION]\n# Name of the source platform\nSOURCE_PLATFORM=\"FALCON\"\n# Name of the target platform\nTARGET_PLATFORM=\"REDSHIFT\"\n\n[FILE_LOCATIONS]\n# Folder where the source model files will be placed (*.dbs/*.csv)\nSOURCE_MODEL_FOLDER = \"./projects/cmc/input/models/falcon/\"\n# Folder where the target model files will be placed (*.dbs/*.csv)\nTARGET_MODEL_FOLDER = \"./projects/cmc/input/models/target/\"\n# Folder where the business model downloaded from the source platform (falcon) will be placed\nBUSINESS_MODEL_FOLDER = \"./projects/cmc/input/business_model/\"\n# Folder of the source remapping.yaml file\nSRC_YAML_FOLDER = \"./projects/cmc/input/connections_yaml/\"\n# File Spec (folder+name) of the output comparison report\nDDL_COMPARISON_REPORT = './projects/cmc/output/ddl_comparison.txt'\n# File Spec (folder+name) of the output remapped yaml file\nTAR_YAML_FILE_NAME = \"./projects/cmc/output/remapped_org_cmc_dummy_connection.yaml\"\n# File Spec (folder+name) of the override csv file\nMANUAL_OVERRIDES = \"./projects/cmc/output/mapping_overrides.csv\"\n\n[MODEL_VALIDATION]\n# Names of source databases which will be completed excluded from the process\nEXCLUDE_DATABASES = ['thoughtspot_internal','thoughtspot_internal_stats','34ae4719-2593-4ed4-ac3e-0ac797f0e7af']\n# Names of source schemas which will be completed excluded from the process\nEXCLUDE_SCHEMAS = [\"ViewDatabase.ViewSchema\"]\n# Do we ignore the length on any Falcon varchar data type\nIGNORE_FALCON_VARCHAR_PRECISION = true\n# Do we accept target data types with a larger length and/or precision?\nACCEPT_LARGER_TARGET_LENGTH_DECIMAL = true\n# In the output report do we only list tables which have actual issues\nONLY_REPORT_TABLES_WITH_ISSUES = true\n# The minimum percentage of column similarity to be required\nMIN_COLUMN_SIMILARITY_PCT=20\n# The minimum amount of columns a table needs to have for a successful column match\nMIN_COL_COUNT_FUZZY=2\n# Width of the report (and terminal)\nREPORT_WIDTH_COLS=160\n# If set True it will populate target definition in the override file with value of source (and target data type)\nCOPY_SOURCE_DEF_WHEN_NOT_FOUND=false\n# Minimum score required for a successful match\nMINIMUM_SCORE_FOR_MATCH=15\n\n[DELTA_MIGRATION]\nSOURCE_TS_URL = \"https://123.456.789\"\nSOURCE_USERNAME = \"username\"\nSOURCE_PASSWORD = \"password\"\nDEST_TS_URL = \"https://789.456.123\"\nDEST_USERNAME = \"username\"\nDEST_PASSWORD = \"password\"\n</code></pre>"},{"location":"migration-tools/create-config/readme/#cli-preview","title":"CLI preview","text":"create_config --help"},{"location":"migration-tools/deltas/readme/","title":"Deltas","text":"<p>The <code>deltas</code> function identifies all created and modified objects since the specified backup date and stores the metadata information into separate subfolders (./projects/<code>yourprojectname</code>/output/delta_migration/...).  These files are the basis for all further <code>migration_tools</code>.</p> <pre><code>usage: deltas [-h] [--cfg-name CONFIGNAME] [--backup-date DATE]\n\n\narguments:\n  -h, --help          show this help message and exit\n  --cfg-name          Name of the config file\n  --backup-date       Date when DL backup was taken (i.e. 2022-12-31).\n</code></pre>"},{"location":"migration-tools/deltas/readme/#cli-preview","title":"CLI preview","text":"deltas --help"},{"location":"migration-tools/groups/readme/","title":"\ud83d\udd10 Create Groups","text":"<p>The <code>create_groups</code> function will create all missing users by looking at the <code>deltas</code> output. </p> <pre><code>usage: create_groups [-h] [--cfg-name CONFIGNAME] [--validate-only yes/no]\n\n\narguments:\n  -h, --help          show this help message and exit\n  --cfg-name          Name of the config file\n  --validate-only     yes/no \n</code></pre>"},{"location":"migration-tools/groups/readme/#cli-preview","title":"CLI preview","text":"create_groups --help"},{"location":"migration-tools/liveboards/readme/","title":"Migrate Liveboards","text":"<p>This function migrates all the created/modified liveboards that have been gathered during the <code>deltas</code> step.</p> <pre><code>usage: migrate_liveboards [-h] [--cfg-name] [--migration-mode MODE]\n                     [--validation-mode TRUE/FALSE] ...\n\narguments:\n  -h, --help                   show this help message and exit\n  --cfg-name   CONFIGNAME      Name of config.toml file.\n  --migration-mode  TYPE       Specify if you want to migrate created or modified answers. valid values:[created, modified]\n  --validation-mode BOOL       Run in validation mode only or apply actual changes (default: True). Set to False to migrate all objects.\n</code></pre>"},{"location":"migration-tools/liveboards/readme/#cli-preview","title":"CLI preview","text":"migrate_liveboards --help"},{"location":"migration-tools/migrate-yaml/readme/","title":"Migrate YAML","text":"<p>This command will perform the actual remapping of the connections YAML file. It will always perform a final validations step before it starts migrating and it requires the validation to be successful and all overrides properly set in the overrides.csv file. The end result is a remapped connection file which will be placed in the output folder.</p>"},{"location":"migration-tools/migrate-yaml/readme/#cli-preview","title":"CLI preview","text":"migrate_yaml --help"},{"location":"migration-tools/ownership/readme/","title":"Transfer Ownership","text":"<p>This function transfers the ownership for all new created objects (answers, liveboards and worksheets) from tsadmin to the original author. Before running this command make sure you ran <code>cs_tools searchable</code> on the falcon instance and store the output files in ./projects/--config-name--/output/delta_migration/cs_tools_falcon/. Don't change any file names in this folder. </p> <pre><code>usage: transfer_ownership [-h] [--cfg-name] \n\narguments:\n  -h, --help                   show this help message and exit\n  --cfg-name   CONFIGNAME      Name of config.toml file.\n</code></pre>"},{"location":"migration-tools/share-permissions/readme/","title":"Share Permissions","text":"<p>This function shares all new answers, liveboards and worksheets with users and groups. Before running this command make sure you ran <code>cs_tools searchable</code> on the falcon instance and store the output files in ./projects/--config-name--/output/delta_migration/cs_tools_falcon/. Don't change any file names in this folder. </p> <pre><code>usage: share_permissions [-h] [--cfg-name] \n\narguments:\n  -h, --help                   show this help message and exit\n  --cfg-name   CONFIGNAME      Name of config.toml file.\n</code></pre>"},{"location":"migration-tools/tags/readme/","title":"Apply Tags","text":"<p>This function synchronizes all tags from the source instance to the target instance. Before running this command make sure you ran <code>cs_tools searchable</code> on the falcon instance and store the output files in ./projects/--config-name--/output/delta_migration/cs_tools_falcon/. Don't change any file names in this folder. </p> <pre><code>usage: apply_tags [-h] [--cfg-name] \n\narguments:\n  -h, --help                   show this help message and exit\n  --cfg-name   CONFIGNAME      Name of config.toml file.\n</code></pre>"},{"location":"migration-tools/users/readme/","title":"\ud83d\udd10 Create Users","text":"<p>The <code>create_users</code> function will create all missing users by looking at the <code>deltas</code> output. </p> <pre><code>usage: create_users [-h] [--cfg-name CONFIGNAME] [--validate-only yes/no]\n\n\narguments:\n  -h, --help          show this help message and exit\n  --cfg-name          Name of the config file\n  --validate-only     yes/no \n</code></pre>"},{"location":"migration-tools/users/readme/#cli-preview","title":"CLI preview","text":"create_users --help"},{"location":"migration-tools/validate-models/readme/","title":"Validate Models","text":"<p>The validate models command allows you to very quickly compare the source model to the target model, often in seconds.</p>"},{"location":"migration-tools/validate-models/readme/#required-inputs","title":"Required Inputs","text":"<p>The command requires three types of input:</p> <ul> <li>Source DDL(s) with extension .sql placed in the /input/ddl/falcon folder or CSV(s) directly placed in the /input/model/falcon folder.</li> <li>Target DDL(s) with extension .sql placed in the /input/ddl/target folder or CSV(s) directly placed in the /input/model/target folder.</li> <li>Source ThoughtSpot Business Model, placed in the /input/business_model folder</li> </ul>"},{"location":"migration-tools/validate-models/readme/#preparation","title":"Preparation","text":"Using DDL scriptsUsing csv files    <p>You can only use DDL scripts as import if you have DBSchema installed.</p> <p>To use DDL scripts as input you would need to have license to DBSchema and have installed as this will be used to convert the DDLs into a DBSchema model file, which is then processed by validate models command. If you don't have DBSchema, you can use CSV files as input and place them in the input/model/ folder.  <p>Before you can run the validate_models or migrate_yaml command, you will need to convert the DDL files to model files. See the convert_ddl command for more information on how to do this.</p>  <p>DDL scripts can only contain content of 1 database</p> <p>When using DDL scripts, each DDL script can only contain the content of 1 database. The name of the script will be used as the database name.</p>    <p>If you don't have a licensed copy of DBSchema, you can just use CSV files which provide the same functionality. The CSV files do not have headers and contain the following information</p>    Column Description Example     Database Name Name of the database my_database   Schema Name Name of the schema my_schema   Table Name Name of the table my_table   Column Name Name of the column my_column   Base Data Type The data type of the column NUMBER   Data Type Length The length of the data type 18   Data Type Precision The precision of the data type 3    <p>For example: <pre><code>my_database,my_schema,my_table,my_column,NUMBER,18,3\n</code></pre></p>  <p>CSV files can contain multiple databases</p> <p>Unlike when using DDL files, CSV files have the database name inside the row data and therefore can contain content from multiple databases.</p>"},{"location":"migration-tools/validate-models/readme/#cli-preview","title":"CLI preview","text":"validate_models --help"},{"location":"migration-tools/worksheets/readme/","title":"Migrate Worksheets","text":"<p>This function migrates all the created/modified worksheets that have been gathered during the <code>deltas</code> step.</p> <pre><code>usage: migrate_worksheets [-h] [--cfg-name] [--migration-mode MODE]\n                     [--validation-mode TRUE/FALSE] ...\n\narguments:\n  -h, --help                   show this help message and exit\n  --cfg-name   CONFIGNAME      Name of config.toml file.\n  --migration-mode  TYPE       Specify if you want to migrate created or modified answers. valid values:[created, modified]\n  --validation-mode BOOL       Run in validation mode only or apply actual changes (default: True). Set to False to migrate all objects.\n</code></pre>"},{"location":"tutorial/config/","title":"Creating a configuration file","text":"<p>Whenever you are starting a new migration project, there are two steps you need to take to prepare your enviroment and set up your project:</p> <pre><code>1. Set up the directory structure and create a template project configuration file\n2. Set/Adjust the settings in the project configuration file according to the needs of your project\n</code></pre>"},{"location":"tutorial/config/#setting-up-the-directory-structure-and-template-configuration-file","title":"Setting up the directory structure and template configuration file","text":"<p>There is a command in migration_tools, which will take care of that for you and that is the create_config command. Please see the command create-config for more details on how this command works.</p> <p>You can run the following command to set up the structure and template configuration file for a project called myfirstproject</p> <pre><code>migration_tools create_config --cfg-name myfirstproject\n</code></pre> <p>If everything went well, the following message will be presented on your screen: </p> <p>The directory structure has now been created and a template configuration file has been created.</p>"},{"location":"tutorial/config/#configuring-for-model-validation-and-yaml-migration","title":"Configuring for model validation and YAML migration","text":"<p>In the folder projects/myfirstproject/config/ a template configuration file has been created for your project with the name myfirstproject.toml. You will now need to tweak these settings for the needs of your project. Please see the command create-config for more details on the various configuration settings.  Typically, however, for model validations and YAML migration, you probably only need to change the source and target platform.</p> <p>In this example we are doing a migration from Falcon to Redshift, so we set those two parameters on top of the configuration file and leave the rest as is.</p> <pre><code>[MIGRATION]\n# Name of the source platform\nSOURCE_PLATFORM=\"FALCON\"\n# Name of the target platform\nTARGET_PLATFORM=\"REDSHIFT\"\n</code></pre> <p>With these configuration setting made, we are ready to do some model validations.</p>"},{"location":"tutorial/config/#configuring-for-delta-migrations-etc","title":"Configuring for delta migrations etc","text":"<p>In the folder projects/myfirstproject/config/ a template configuration file has been created for your project with the name myfirstproject.toml. You will now need to tweak these settings for the needs of your project. Please see the command create-config for more details on the various configuration settings.  In order to be able to run delta migrations you will need to fill in the delta migration parameters like source/destination url, credentials etc.</p> <p>In this example we are doing a migration from Falcon to Cloud, so we set the SOURCE_TS_URL to the URL of our FALCON instance and the DEST_TS_URL to the URL of the cloud instance. </p> <p>In general we can migrate tml objects from any TS instance to another. </p> <pre><code>[DELTA_MIGRATION]\nSOURCE_TS_URL = \"https://mycompany.thoughtspot.com\"\nSOURCE_USERNAME = \"tsadmin\"\nSOURCE_PASSWORD = \"password\"\nDEST_TS_URL = \"https://mycompany.thoughtspot.cloud\"\nDEST_USERNAME = \"tsadmin\"\nDEST_PASSWORD = \"password\"\n</code></pre>"},{"location":"tutorial/delta_migration/","title":"Running a delta migration","text":"<p>Before you start a delta migration make sure users and groups are in sync between both instances.  cs_tools user-management sync can help you to achieve this by exporting all users and groups including their assignments and importing those to the new environment.</p> <ol> <li>compare source and target instance by running the deltas command. This function looks at the created and modified time stamp of the metadata objects and stores them into separate subfolders.</li> <li>run cs_tools searchable on the source instance in order to gather all necessary metadata information and store the output in ./projects/myproject/output/delta_migration/cs_tools_falcon</li> <li>migrate worksheets first, as there are maybe dependencies to answers and liveboards. Run all commands in validation mode first and check log files for potential migration issues (version incompatibility).</li> <li>now migrate answers and liveboards (other object types coming soon). </li> <li>Failed objects will be store in ./projects/myproject/output/delta_migration/failed_<code>object type</code>.csv</li> <li>migrate all failed objects from all object types manually or try to fix them in the source system. If you migrate objects manually store their new guid in the failed.csv file. This helps to do reduce manual effort during the next steps. </li> <li>apply tags for all objects</li> <li>transfer ownership of all objects </li> <li>share permissions with users and groups (this will only impact new objects, permission delta will be available in the upcoming version of <code>ps_migration_tools</code>)</li> </ol>"},{"location":"tutorial/install/","title":"Getting started with migration_tools","text":""},{"location":"tutorial/install/#cli-preview","title":"CLI preview","text":"migration_tools --help"},{"location":"tutorial/intro/","title":"Getting started with migration_tools","text":""},{"location":"tutorial/intro/#cli-preview","title":"CLI preview","text":"migration_tools --help"},{"location":"tutorial/remapping/","title":"Remapping a Connections File","text":""},{"location":"tutorial/remapping/#prerequisites","title":"Prerequisites","text":"<p>To be able to remap the connections, we need of course the original remapping file. This original yaml file needs to be downloaded from the new target cluster after the migration pipeline has completed and the backup has been restored. This this will need to be placed in the /projects/myfirstproject/input/connections_yaml folder.</p>  <p>You see that it first does a validation again. The reason for this is to make sure everything is still valid and during that validation it will collect the mapping rules it needs to be able to perform the remapping. It also raised two warnings. The first warning is to inform you that there are a number of tables in the YAML file with very similar definitions. This is just to let you know it might be possible that incorrect mappings might be made (in case when two tables are completely identical). The other warning is that it has identified tables in the YAML file which did not exist in our original source model. For the user uploaded tables this is typical, as due to an issue in the migration pipeline, they will have different names, but don't worry the migration tools will fix that.</p> <p>And finally you see that the remapping has failed. It still encountered an issue, which needs to be resolved. We will need to open the override file to fix this.</p>  <p>At the bottom of the file you see a few new entries with YAML in the first column, which are unmapped. It was not able to locate the table rpt_ga_topline_monthly directly initially, but was able to map it to RPT_GA_TOPLINE_MONTHLY_ALTERED via the same fuzzy matching process. You might have noticed that this table was already mentioned in one of the warnings earlier. So if we are happy with this mapping we can just switch the status to OVERRIDE to accept them.</p> <p>Now we can just rerun the process.</p>  <p>And everything went through now and it stored the remappings in a file called remapped_connection.yaml</p>"},{"location":"tutorial/validate/","title":"Running a Model validation","text":"<p>Make sure you have configured migration_tools before you run this step.</p> <p>For the validation process to run smoothly, make sure you have setup and configured migration_tools for your project, as described in Setup a Config File</p>"},{"location":"tutorial/validate/#prerequisites","title":"Prerequisites","text":"<p>To run a validation we need three inputs:</p> <pre><code>* A source model, which can be a CSV or DBS file, but we are using a CSV file here\n* A target model, which can be a CSV or DBS file, but we are using a CSV file here\n* The ThoughtSpot business model downloaded from the source platform\n</code></pre> <p>We will place the falcon.csv, which is the source definition file in the /projects/myfirstproject/input/models/falcon folder. Similarly we place the redshift.csv file in the /projects/myfirstproject/input/models/target folder. Finally, we place our downloaded source business model (xls) in the /projects/myfirstproject/input/business_model folder.</p>"},{"location":"tutorial/validate/#running-the-validation","title":"Running the validation","text":"<p>We are now ready to perform our first validation run, by executing the validate_models command: <pre><code>poetry run migration_tools validate_models --cfg-name myfirstproject\n</code></pre></p> <p>Which will render the following output on the screen:</p>  <p>We see an error is detected straight away, and that is that an incorrect data type has been encountered in the target definition. This is the only case where the process halts immediatly as it is a critical error and prevents the process from parsing the structures correctly. If the target models are correctly extracted by the client, this should not really happen, but sometimes people cut corners or copy and paste and that is why this protection is built-in.</p> <p>It is reported that the column TS_CREATE_TIMESTAMP in table thoughtspot_db.cmc_data_owner.EDW_IL_INSTRUMENTS_D has been defined as DATETIME. This is not a valid data type for RedShift (should be TIMESTAMP). The action to take here is notify the client and find out how this could have happened. At the same time you can correct it yourself in the target definition by changing it to the appropriate TIMESTAMP data type.</p> <p>We can now run the validation again.</p>  <p>What we can see here is:</p> <pre><code>- It successfully loaded the configuration file for the myfirstproject project\n- It parsed both input csv files successfully\n- It executed the mapping process, which completed in 2.4 seconds\n- It detected 13 issues, which have been documented in the report ddl_comparison.txt\n- It also created an overwrite file in which the suggested mappings are document\n</code></pre>"},{"location":"tutorial/validate/#reviewing-the-mapping-report","title":"Reviewing the mapping report","text":"<p>Lets examine the mapping report in more detail by opening it and see what the issues are. On top of the report we can see in the header that we have performed a validation between a Falcon source platform and a Redshift target platform.</p>  <p>If we scroll a bit further down we see an overview of what has been analysed in terms of number of objects.</p>  <p>We see here that in total the validation process analysed 3 databases, 3 schemas and 167 columns in 2.4 seconds.</p> <p>The next section will show you all project mappings, which includes all tables which could be directly mapped or which the process was able to map based on fuzzy mapping and similarity. It will also list the score of each mapping (0-100).</p>  <p>The majority of mappings have a score of 100.0, which means they were a perfect match. However, there are 3 mappings with a score lower than 100.0 and even 10 with a score of 0.0. These are mappings we will need to investigate. Below this table in the report it also gives you a clarification of the scoring system, which is as follows:</p>    Score Explanation     100.0 Perfect Match:    - Same table name (case insensitive)    - Same columns (fuzzy matched) and same number of columns   75.0 - 85.0 - Different table names    - Same columns (fuzzy matched) and same number of columns    - The closer the number is to 85, the more similar the two table names are   50.0 - Same table name (case insensitive)    - Source columns are present in target table, but target table has additional columns   25.0 - 35.0 - Different table names    - Source columns are present in target table, but target table has additional columns    - The closer the number is to 35, the more similar the two table names are   0.0 No suitable match found    <p>The next table in the report lists all the tables with issues:</p>  <p>And here we can see that there were in fact 14 issues, 13 we already knew about from the mapping scores, but the process also detected something else. Now we can investigate in detail, what all the issues actually are. All the subsequent sections in this report are detailed results per table. We are going to start with the first issue report for mapping table DEC_ENG_RAF_LIST.</p>  <p>We see here that it is only reported as a minor issue. And the issue is that the process could not find a perfect match, but via its fuzzy matching algorithm it did find a good candidate. It found a table which has a different name DEC_ENG_RAF_LIST_ALTERED, but has the same matching columns. It generated a score of 83, as the initial score was 75, but that has been increased based on the similarity of the table name.</p> <p>If we are happy, with this suggestion we can confirm it by marking it in the override file and run the process again. How to mark suggestions in the override file we will see in the next section, after we have reviewed all the issues.</p> <p>For now, we going to look at the next issue, which was with table EDW_IL_IDENTITIES_D.</p>  <p>This is also a minor issue as it could not find a perfect match. However, it is slightly different than the previous one. In this case it did find a match on table name and similarity in columns, however, the target column has additional columns which do not exist in the source. This is a valid situation and can be successfully remapped, but it is always recommended to review them.</p> <p>Similar as before, if we are happy with this mapping, we can confirm it later in the override file.</p> <p>If we look at the final minor issue, we see it is a similar issue as the previous one, so we can deal with it in the same way.</p>  <p>Now let's look at the RPT_DEBT table, as it was reported there was a problem there too:</p>  <p>Note that this mapping still has a score of 100 as it was able to work out what the correct target table is, but there is a different issue here and that is that the data types don't match. The column BUSINESS_DATE is defined as DATE on the source model, but on the target it is defined as a VARCHAR(100) and these are not compatible. This is probably the most common issue in validations and this command will find them all.</p> <p>The resolution here is to inform the client that they will need to change the target data type. And we can confirm this change in the override file later.</p> <p>Now let's move on to the severe issues. The first one is reported on FalconUserDataDataBase.FalconUserDataSchema.USERDATA-00790dd7-b08d-4b4c-b895-1501183a3fa9, which is a user uploaded table.</p>  <p>The issue is clear here. The process was not able to find any suitable match for this table, which means there is not a table which even looks a little bit like this table, so it is most likely missing on the target system. The solution here is to notify the client that this table is missing and they will need to create it.  However, if you can agree the name with the client, you can already specifiy this in the override file and continue your validation.</p> <p>If we review the remaining 9 severe issues, we can see they are the same issue (missing table), which means we can deal with those the same way as the previous one.</p>"},{"location":"tutorial/validate/#fixing-issues-and-confirming-suggested-mappings-in-the-override-file","title":"Fixing issues and confirming suggested mappings in the override file","text":"<p>We have done our first run now and identified all the initial issues. We don't have necessarily to wait for the client to make all the changes, we can do it ourselves (as long as the client will stick to making the identical changes). For one issue (the incompatible data type on RPT_DEBT) we need to fix the target model input file, which we can do easily by amending the csv file.</p> <p>So open the target model file (redshift.csv in our case), find the column and correct the data type.</p> <p>To confirm all other suggested changes, we can do this in the mapping_overrides.csv file. You can open this file with your favourite spreadsheet editor and it will look similar as the image below.</p>  <p>The override contains the following columns:</p>    Column Name Description     mapping_category DDL or YAML, depends on which process added this mapping row   mapping_type Either TABLE or COLUMN, TABLE for just the table mapping, COLUMN for the column mappings   src_database Name of the source database   src_schema Name of the source schema   src_table Name of the source table   src_column Name of the source column   src_datatype Data type of the source column   src_datatype_length Length of the source column data type   src_datatype_decimal Precision of the source column data type   tar_database Name of the target database   tar_schema Name of the target schema   tar_table Name of the target table   tar_column Name of the target column   tar_datatype Data type of the target column   tar_datatype_length Length of the target column data type   tar_datatype_decimal Precision of the target column data type   status Status of the mapping, default UNMAPPED, change to OVERRIDE to accept the mapping on this line   rank Score of the mapping (only set for tables)    <p>Basically each row contains a source to target mapping. For a table there will be a row for the table mapping and rows for each column. Where a mapping has been identified by the process it will have populated the target already. For tables where no match has been found, the target part of the table and column mappings will be left empty. The override file will only list the mappings with a score lower than 100, i.e. the mappings which need to be fixed or confirmed.</p> <p>To deal with the identified issues, we can do the following:</p> <pre><code>1. For the issues with tables DEC_ENG_RAF_LIST, EDW_IL_IDENTITIES_D and EDW_IL_PARTNERS_D we can just confirm the mappings by changing the value in the status column to OVERRIDE to accept the suggested mappings.\n2. For the issue with RPT_DEBT (the incorrect data type), we can fix the data type by setting the target data type to DATE and set the mapping status to OVERRIDE\n3. For the missing tables - assuming that you have agreed with the client that they will create them with the exact same names, columns and data types, we need to do the following: \n   1. Populate the target fields by copying the source table, columns and data types to the target fields.\n   2. Make sure the target data types are supported by the target system and change them where necessary\n   3. Agree with the client the name of the target database/schema where these objects will be created and set these in the target database and target schema column\n   4. Change the mapping status to OVERRIDE for these rows\n</code></pre>"},{"location":"tutorial/validate/#rerunning-the-validation-with-overrides","title":"Rerunning the validation with overrides","text":"<p>We can now rerun the validation.</p>  <p>And the validation passed, because we have fixed/overridden the issues and we are ready to remap to connections file. </p>"}]}